{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Classification App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "* https://www.w3schools.com/colors/colors_picker.asp\n",
    "* https://stackoverflow.com/questions/47949173/deploy-a-python-app-to-heroku-using-conda-environments-instead-of-virtualenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When deploying, remember to change Bs4 `html5lib` to `html.parser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import punkt\n",
    "from nltk.corpus.reader import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import dash_table\n",
    "import dash_renderer\n",
    "from dash.dependencies import Input, Output, State\n",
    "import plotly.graph_objs as go\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performing model is the SVM. We'll use it in the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:253: UserWarning:\n",
      "\n",
      "Trying to unpickle estimator SVC from version 0.19.1 when using version 0.20.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_models = \"C:/Users/migue/Data Science/Master Data Science/KSCHOOL/9. TFM/0. Latest News Classifier/04. Model Training/Models/\"\n",
    "\n",
    "# SVM\n",
    "path_svm = path_models + 'best_svc.pickle'\n",
    "with open(path_svm, 'rb') as data:\n",
    "    svc_model = pickle.load(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. TF-IDF object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:253: UserWarning:\n",
      "\n",
      "Trying to unpickle estimator TfidfTransformer from version 0.19.1 when using version 0.20.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:253: UserWarning:\n",
      "\n",
      "Trying to unpickle estimator TfidfVectorizer from version 0.19.1 when using version 0.20.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_tfidf = \"C:/Users/migue/Data Science/Master Data Science/KSCHOOL/9. TFM/0. Latest News Classifier/03. Feature Engineering/Pickles/tfidf.pickle\"\n",
    "\n",
    "with open(path_tfidf, 'rb') as data:\n",
    "    tfidf = pickle.load(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Category mapping dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_codes = {\n",
    "    'business': 0,\n",
    "    'entertainment': 1,\n",
    "    'politics': 2,\n",
    "    'sport': 3,\n",
    "    'tech': 4,\n",
    "    'other':5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Definition of functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Web Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El Pais\n",
    "def get_news_elpais():\n",
    "    \n",
    "    # url definition\n",
    "    url = \"https://elpais.com/elpais/inenglish.html\"\n",
    "    \n",
    "    # Request\n",
    "    r1 = requests.get(url)\n",
    "    r1.status_code\n",
    "\n",
    "    # We'll save in coverpage the cover page content\n",
    "    coverpage = r1.content\n",
    "\n",
    "    # Soup creation\n",
    "    soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "\n",
    "    # News identification\n",
    "    coverpage_news = soup1.find_all('h2', class_='articulo-titulo')\n",
    "    len(coverpage_news)\n",
    "    \n",
    "    # We have to delete elements such as albums and other things\n",
    "    coverpage_news = [x for x in coverpage_news if \"inenglish\" in str(x)]\n",
    "    \n",
    "    number_of_articles = 5\n",
    "\n",
    "    # Empty lists for content, links and titles\n",
    "    news_contents = []\n",
    "    list_links = []\n",
    "    list_titles = []\n",
    "\n",
    "    for n in np.arange(0, number_of_articles):\n",
    "                \n",
    "        # Getting the link of the article\n",
    "        link = coverpage_news[n].find('a')['href']\n",
    "        list_links.append(link)\n",
    "\n",
    "        # Getting the title\n",
    "        title = coverpage_news[n].find('a').get_text()\n",
    "        list_titles.append(title)\n",
    "\n",
    "        # Reading the content (it is divided in paragraphs)\n",
    "        article = requests.get(link)\n",
    "        article_content = article.content\n",
    "        soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "        body = soup_article.find_all('div', class_='articulo-cuerpo')\n",
    "        x = body[0].find_all('p')\n",
    "\n",
    "        # Unifying the paragraphs\n",
    "        list_paragraphs = []\n",
    "        for p in np.arange(0, len(x)):\n",
    "            paragraph = x[p].get_text()\n",
    "            list_paragraphs.append(paragraph)\n",
    "            final_article = \" \".join(list_paragraphs)\n",
    "\n",
    "        news_contents.append(final_article)\n",
    "        \n",
    "    # df_features\n",
    "    df_features = pd.DataFrame(\n",
    "         {'Content': news_contents \n",
    "        })\n",
    "\n",
    "    # df_show_info\n",
    "    df_show_info = pd.DataFrame(\n",
    "        {'Article Title': list_titles,\n",
    "         'Article Link': list_links,\n",
    "         'Newspaper': 'El Pais English'})\n",
    "    \n",
    "    return (df_features, df_show_info)\n",
    "\n",
    "# The Guardian\n",
    "def get_news_theguardian():\n",
    "    \n",
    "    # url definition\n",
    "    url = \"https://www.theguardian.com/uk\"\n",
    "    \n",
    "    # Request\n",
    "    r1 = requests.get(url)\n",
    "    r1.status_code\n",
    "\n",
    "    # We'll save in coverpage the cover page content\n",
    "    coverpage = r1.content\n",
    "\n",
    "    # Soup creation\n",
    "    soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "\n",
    "    # News identification\n",
    "    coverpage_news = soup1.find_all('h3', class_='fc-item__title')\n",
    "    len(coverpage_news)\n",
    "    \n",
    "    # We have to delete elements such as albums and other things\n",
    "    coverpage_news = [x for x in coverpage_news if \"live\" not in str(x)]\n",
    "    coverpage_news = [x for x in coverpage_news if \"commentisfree\" not in str(x)]\n",
    "    coverpage_news = [x for x in coverpage_news if \"ng-interactive\" not in str(x)]\n",
    "\n",
    "    \n",
    "    number_of_articles = 5\n",
    "\n",
    "    # Empty lists for content, links and titles\n",
    "    news_contents = []\n",
    "    list_links = []\n",
    "    list_titles = []\n",
    "\n",
    "    for n in np.arange(0, number_of_articles):\n",
    "\n",
    "        # Getting the link of the article\n",
    "        link = coverpage_news[n].find('a')['href']\n",
    "        list_links.append(link)\n",
    "\n",
    "        # Getting the title\n",
    "        title = coverpage_news[n].find('a').get_text()\n",
    "        list_titles.append(title)\n",
    "\n",
    "        # Reading the content (it is divided in paragraphs)\n",
    "        article = requests.get(link)\n",
    "        article_content = article.content\n",
    "        soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "        body = soup_article.find_all('div', class_='content__article-body from-content-api js-article__body')\n",
    "        x = body[0].find_all('p')\n",
    "\n",
    "        # Unifying the paragraphs\n",
    "        list_paragraphs = []\n",
    "        for p in np.arange(0, len(x)):\n",
    "            paragraph = x[p].get_text()\n",
    "            list_paragraphs.append(paragraph)\n",
    "            final_article = \" \".join(list_paragraphs)\n",
    "\n",
    "        news_contents.append(final_article)\n",
    "\n",
    "    # df_features\n",
    "    df_features = pd.DataFrame(\n",
    "         {'Content': news_contents \n",
    "        })\n",
    "\n",
    "    # df_show_info\n",
    "    df_show_info = pd.DataFrame(\n",
    "        {'Article Title': list_titles,\n",
    "         'Article Link': list_links,\n",
    "         'Newspaper': 'The Guardian'})\n",
    "\n",
    "    \n",
    "    return (df_features, df_show_info)\n",
    "\n",
    "# Sky News\n",
    "def get_news_skynews():\n",
    "    \n",
    "    # url definition\n",
    "    url = \"https://news.sky.com/us\"\n",
    "\n",
    "    # Request\n",
    "    r1 = requests.get(url)\n",
    "\n",
    "    # We'll save in coverpage the cover page content\n",
    "    coverpage = r1.content\n",
    "\n",
    "    # Soup creation\n",
    "    soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "\n",
    "    # News identification\n",
    "    coverpage_news = soup1.find_all('h3', class_=\"sdc-site-tile__headline\")\n",
    "\n",
    "    number_of_articles = 5\n",
    "\n",
    "    # Empty lists for content, links and titles\n",
    "    news_contents = []\n",
    "    list_links = []\n",
    "    list_titles = []\n",
    "\n",
    "    for n in np.arange(0, number_of_articles):\n",
    "\n",
    "        # Getting the link of the article\n",
    "        link = \"https://news.sky.com\" + coverpage_news[n].find('a', class_='sdc-site-tile__headline-link')['href']\n",
    "        list_links.append(link)\n",
    "\n",
    "        # Getting the title\n",
    "        title = coverpage_news[n].find('a').find('span').get_text()\n",
    "        list_titles.append(title)\n",
    "\n",
    "        # Reading the content (it is divided in paragraphs)\n",
    "        article = requests.get(link)\n",
    "        article_content = article.content\n",
    "        soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "        body = soup_article.find_all('div', class_='sdc-article-body sdc-article-body--lead')\n",
    "        x = body[0].find_all('p')\n",
    "\n",
    "        # Unifying the paragraphs\n",
    "        list_paragraphs = []\n",
    "        for p in np.arange(0, len(x)):\n",
    "            paragraph = x[p].get_text()\n",
    "            list_paragraphs.append(paragraph)\n",
    "            final_article = \" \".join(list_paragraphs)\n",
    "\n",
    "        news_contents.append(final_article)\n",
    "\n",
    "    # df_features\n",
    "    df_features = pd.DataFrame(\n",
    "         {'Content': news_contents \n",
    "        })\n",
    "\n",
    "    # df_show_info\n",
    "    df_show_info = pd.DataFrame(\n",
    "        {'Article Title': list_titles,\n",
    "         'Article Link': list_links,\n",
    "         'Newspaper': 'Sky News'})\n",
    "    \n",
    "    return (df_features, df_show_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_signs = list(\"?:!.,;\")\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "def create_features_from_df(df):\n",
    "    \n",
    "    df['Content_Parsed_1'] = df['Content'].str.replace(\"\\r\", \" \")\n",
    "    df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"\\n\", \" \")\n",
    "    df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"    \", \" \")\n",
    "    df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace('\"', '')\n",
    "    \n",
    "    df['Content_Parsed_2'] = df['Content_Parsed_1'].str.lower()\n",
    "    \n",
    "    df['Content_Parsed_3'] = df['Content_Parsed_2']\n",
    "    for punct_sign in punctuation_signs:\n",
    "        df['Content_Parsed_3'] = df['Content_Parsed_3'].str.replace(punct_sign, '')\n",
    "        \n",
    "    df['Content_Parsed_4'] = df['Content_Parsed_3'].str.replace(\"'s\", \"\")\n",
    "    \n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    nrows = len(df)\n",
    "    lemmatized_text_list = []\n",
    "    for row in range(0, nrows):\n",
    "\n",
    "        # Create an empty list containing lemmatized words\n",
    "        lemmatized_list = []\n",
    "        # Save the text and its words into an object\n",
    "        text = df.loc[row]['Content_Parsed_4']\n",
    "        text_words = text.split(\" \")\n",
    "        # Iterate through every word to lemmatize\n",
    "        for word in text_words:\n",
    "            lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "        # Join the list\n",
    "        lemmatized_text = \" \".join(lemmatized_list)\n",
    "        # Append to the list containing the texts\n",
    "        lemmatized_text_list.append(lemmatized_text)\n",
    "    \n",
    "    df['Content_Parsed_5'] = lemmatized_text_list\n",
    "    \n",
    "    df['Content_Parsed_6'] = df['Content_Parsed_5']\n",
    "    for stop_word in stop_words:\n",
    "        regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
    "        df['Content_Parsed_6'] = df['Content_Parsed_6'].str.replace(regex_stopword, '')\n",
    "        \n",
    "    df = df['Content_Parsed_6']\n",
    "    df = df.rename(columns={'Content_Parsed_6': 'Content_Parsed'})\n",
    "    \n",
    "    # TF-IDF\n",
    "    features = tfidf.transform(df).toarray()\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_name(category_id):\n",
    "    for category, id_ in category_codes.items():    \n",
    "        if id_ == category_id:\n",
    "            return category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Prediction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_features(features):\n",
    "        \n",
    "    # Obtain the highest probability of the predictions for each article\n",
    "    predictions_proba = svc_model.predict_proba(features).max(axis=1)    \n",
    "    \n",
    "    # Predict using the input model\n",
    "    predictions_pre = svc_model.predict(features)\n",
    "\n",
    "    # Replace prediction with 6 if associated cond. probability less than threshold\n",
    "    predictions = []\n",
    "\n",
    "    for prob, cat in zip(predictions_proba, predictions_pre):\n",
    "        if prob > .65:\n",
    "            predictions.append(cat)\n",
    "        else:\n",
    "            predictions.append(5)\n",
    "\n",
    "    # Return result\n",
    "    categories = [get_category_name(x) for x in predictions]\n",
    "    \n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_df(df, categories):\n",
    "    df['Prediction'] = categories\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the whole process can be written in these 4 lines of code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Get the scraped dataframes\n",
    "df_features, df_show_info = get_news_elpais()\n",
    "\n",
    "# Create features\n",
    "features = create_features_from_df(df_features)\n",
    "\n",
    "# Predict\n",
    "predictions = predict_from_features(features)\n",
    "\n",
    "# Put into dataset\n",
    "df = complete_df(df_show_info, predictions)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dash App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stylesheet\n",
    "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "\n",
    "app = dash.Dash(__name__, external_stylesheets=external_stylesheets)\n",
    "\n",
    "# Colors\n",
    "colors = {\n",
    "    'background': '#ECECEC',  \n",
    "    'text': '#696969',\n",
    "    'titles': '#599ACF',\n",
    "    'blocks': '#F7F7F7',\n",
    "    'graph_background': '#F7F7F7',\n",
    "    'banner': '#C3DCF2'\n",
    "\n",
    "}\n",
    "\n",
    "# Markdown text\n",
    "markdown_text1 = '''\n",
    "\n",
    "This application gathers the latest news from the newspapers **El Pais**, **The Guardian** and **Sky News**, predicts their category between **Politics**, **Business**, **Entertainment**, **Sport**, **Tech** and **Other** and then shows a summary.\n",
    "\n",
    "The scraped news are converted into a numeric feature vector with *TF-IDF vectorization*. Then, a *Support Vector Classifier* is applied to predict each category.\n",
    "\n",
    "This app is meant for didactic purposes.\n",
    "\n",
    "Please enter which newspapers would you like to scrape news off and press the **Scrape** button.\n",
    "\n",
    "'''\n",
    "\n",
    "markdown_text2 = '''\n",
    "\n",
    " Created by Miguel Fern√°ndez Zafra. Visit my webpage at [mfz.es](https://www.mfz.es/) and the [github repo](https://github.com/miguelfzafra/Latest-News-Classifier).\n",
    "\n",
    " *Disclaimer: this app is not under periodic maintenance. A live web-scraping process is carried out every time you run the app, so there may be some crashes due to the failing status of some requests.*\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "app.layout = html.Div(style={'backgroundColor':colors['background']}, children=[\n",
    "    \n",
    "    # Space before title\n",
    "    html.H1(children=' ',\n",
    "            style={'padding': '10px'}\n",
    "           ),\n",
    "    \n",
    "    # Title\n",
    "    html.Div(\n",
    "        [\n",
    "            html.H3(children='News Classification App',\n",
    "                    style={\"margin-bottom\": \"0px\"}\n",
    "                   ),\n",
    "            html.H6(children='A Machine Learning based app')\n",
    "        ],\n",
    "        style={\n",
    "            'textAlign': 'center',\n",
    "            'color': colors['text'],\n",
    "            #'padding': '0px',\n",
    "            'backgroundColor': colors['background']\n",
    "              },\n",
    "        className='banner',\n",
    "            ),\n",
    "    \n",
    "\n",
    "    # Space after title\n",
    "    html.H1(children=' ',\n",
    "            style={'padding': '1px'}),\n",
    "\n",
    "\n",
    "    # Text boxes\n",
    "    html.Div(\n",
    "        [\n",
    "            html.Div(\n",
    "                [\n",
    "                    html.H6(children='What does this app do?',\n",
    "                            style={'color':colors['titles']}),\n",
    "                    \n",
    "                    html.Div(\n",
    "                        [dcc.Markdown(children=markdown_text1),],\n",
    "                        style={'font-size': '12px',\n",
    "                               'color': colors['text']}),\n",
    "                                        \n",
    "                    html.Div(\n",
    "                        [\n",
    "                            dcc.Dropdown(\n",
    "                                options=[\n",
    "                                    {'label': 'El Pais English', 'value': 'EPE'},\n",
    "                                    {'label': 'The Guardian', 'value': 'THG'},\n",
    "                                    {'label': 'Sky News', 'value': 'SKN'}\n",
    "                                        ],\n",
    "                                value=['EPE'],\n",
    "                                multi=True,\n",
    "                                id='checklist'),\n",
    "                        ],\n",
    "                        style={'font-size': '12px',\n",
    "                               'margin-top': '25px'}),\n",
    "                    \n",
    "                    html.Div([\n",
    "                        html.Button('Scrape', \n",
    "                                    id='submit', \n",
    "                                    type='submit', \n",
    "                                    style={'color': colors['blocks'],\n",
    "                                           'background-color': colors['titles'],\n",
    "                                           'border': 'None'})],\n",
    "                        style={'textAlign': 'center',\n",
    "                               'padding': '20px',\n",
    "                               \"margin-bottom\": \"0px\",\n",
    "                               'color': colors['titles']}),\n",
    "            \n",
    "                    dcc.Loading(id=\"loading-1\", children=[html.Div(id=\"loading-output-1\")], type=\"circle\"),\n",
    "                    \n",
    "                    html.Hr(),\n",
    "                    html.H6(children='Headlines',\n",
    "                            style={'color': colors['titles']}),\n",
    "\n",
    "                    # Headlines\n",
    "                    html.A(id=\"textarea1a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea1b\", style={'color': colors['text'], 'font-size': '11px'}),\n",
    "                    html.A(id=\"textarea2a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea2b\", style={'color': colors['text'], 'font-size': '11px'}),\n",
    "                    html.A(id=\"textarea3a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea3b\", style={'color': colors['text'], 'font-size': '11px'}),\n",
    "                    html.A(id=\"textarea4a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea4b\", style={'color': colors['text'], 'font-size': '11px'}),\n",
    "                    html.A(id=\"textarea5a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea5b\", style={'color': colors['text'], 'font-size': '11px'}),\n",
    "                    html.A(id=\"textarea6a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea6b\", style={'color': colors['text'], 'font-size': '11px'}),\n",
    "                    html.A(id=\"textarea7a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea7b\", style={'color': colors['text'], 'font-size': '11px'}),\n",
    "                    html.A(id=\"textarea8a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea8b\", style={'color': colors['text'], 'font-size': '11px'}),\n",
    "                    html.A(id=\"textarea9a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea9b\", style={'color': colors['text'], 'font-size': '11px'}),\n",
    "                    html.A(id=\"textarea10a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea10b\", style={'color': colors['text'], 'font-size': '11px'}),\n",
    "                    html.A(id=\"textarea11a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea11b\", style={'color': colors['text'], 'font-size': '11px'}),\n",
    "                    html.A(id=\"textarea12a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea12b\", style={'color': colors['text'], 'font-size': '11px'}),\n",
    "                    html.A(id=\"textarea13a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea13b\", style={'color': colors['text'], 'font-size': '11px'}),\n",
    "                    html.A(id=\"textarea14a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea14b\", style={'color': colors['text'], 'font-size': '11px'}),\n",
    "                    html.A(id=\"textarea15a\", target=\"_blank\", style={'font-size': '12px'}),\n",
    "                    html.P(id=\"textarea15b\", style={'color': colors['text'], 'font-size': '11px'})\n",
    "                                                            \n",
    "                ],\n",
    "                     style={'backgroundColor': colors['blocks'],\n",
    "                            'padding': '20px',\n",
    "                            'border-radius': '5px',\n",
    "                            'box-shadow': '1px 1px 1px #9D9D9D'},\n",
    "                     className='one-half column'),\n",
    "            \n",
    "            html.Div(\n",
    "                [\n",
    "                    html.H6(\"Graphic summary\",\n",
    "                            style={'color': colors['titles']}),\n",
    "\n",
    "                    html.Div([\n",
    "                         dcc.Graph(id='graph1', style={'height': '300px'})\n",
    "                         ],\n",
    "                         style={'backgroundColor': colors['blocks'],\n",
    "                                'padding': '20px'}\n",
    "                    ),\n",
    "                    \n",
    "                    html.Div([\n",
    "                         dcc.Graph(id='graph2', style={'height': '300px'})\n",
    "                         ],\n",
    "                         style={'backgroundColor': colors['blocks'],\n",
    "                                'padding': '20px'}\n",
    "                    )\n",
    "                ],\n",
    "                     style={'backgroundColor': colors['blocks'],\n",
    "                            'padding': '20px',\n",
    "                            'border-radius': '5px',\n",
    "                            'box-shadow': '1px 1px 1px #9D9D9D'},\n",
    "                     className='one-half column')\n",
    "\n",
    "        ],\n",
    "        className=\"row flex-display\",\n",
    "        style={'padding': '20px',\n",
    "               'margin-bottom': '0px'}\n",
    "    ),\n",
    "    \n",
    "        \n",
    "    # Space\n",
    "    html.H1(id='space2', children=' '),\n",
    "        \n",
    "    \n",
    "    # Final paragraph\n",
    "    html.Div(\n",
    "            [dcc.Markdown(children=markdown_text2),],\n",
    "            style={'font-size': '12px',\n",
    "                   'color': colors['text']}),\n",
    "\n",
    "    \n",
    "    # Hidden div inside the app that stores the intermediate value\n",
    "    html.Div(id='intermediate-value', style={'display': 'none'})\n",
    "    \n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    [\n",
    "    Output('intermediate-value', 'children'),\n",
    "    Output('loading-1', 'children')\n",
    "    ],\n",
    "    [Input('submit', 'n_clicks')],\n",
    "    [State('checklist', 'value')])\n",
    "def scrape_and_predict(n_clicks, values):\n",
    "            \n",
    "    df_features = pd.DataFrame()\n",
    "    df_show_info = pd.DataFrame()\n",
    "    \n",
    "    if 'EPE' in values:\n",
    "        # Get the scraped dataframes\n",
    "        df_features = df_features.append(get_news_elpais()[0])\n",
    "        df_show_info = df_show_info.append(get_news_elpais()[1])\n",
    "    \n",
    "    if 'THG' in values:\n",
    "        df_features = df_features.append(get_news_theguardian()[0])\n",
    "        df_show_info = df_show_info.append(get_news_theguardian()[1])\n",
    "        \n",
    "    if 'SKN' in values:\n",
    "        df_features = df_features.append(get_news_skynews()[0])\n",
    "        df_show_info = df_show_info.append(get_news_skynews()[1])\n",
    "\n",
    "    df_features = df_features.reset_index().drop('index', axis=1)\n",
    "    \n",
    "    # Create features\n",
    "    features = create_features_from_df(df_features)\n",
    "    # Predict\n",
    "    predictions = predict_from_features(features)\n",
    "    # Put into dataset\n",
    "    df = complete_df(df_show_info, predictions)\n",
    "    # df.to_csv('Tableau Teaser/df_tableau.csv', sep='^')  # export to csv to work out an example in Tableau\n",
    "    \n",
    "    return df.to_json(date_format='iso', orient='split'), ' '\n",
    "\n",
    "@app.callback(\n",
    "    Output('graph1', 'figure'),\n",
    "    [Input('intermediate-value', 'children')])\n",
    "def update_barchart(jsonified_df):\n",
    "    \n",
    "    df = pd.read_json(jsonified_df, orient='split')\n",
    "    \n",
    "    # Create a summary df\n",
    "    df_sum = df.groupby(['Newspaper', 'Prediction']).count()['Article Title']\n",
    "\n",
    "    # Create x and y arrays for the bar plot for every newspaper\n",
    "    if 'El Pais English' in df_sum.index:\n",
    "    \n",
    "        df_sum_epe = df_sum['El Pais English']\n",
    "        x_epe = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "        y_epe = [[df_sum_epe['politics'] if 'politics' in df_sum_epe.index else 0][0],\n",
    "                [df_sum_epe['business'] if 'business' in df_sum_epe.index else 0][0],\n",
    "                [df_sum_epe['entertainment'] if 'entertainment' in df_sum_epe.index else 0][0],\n",
    "                [df_sum_epe['sport'] if 'sport' in df_sum_epe.index else 0][0],\n",
    "                [df_sum_epe['tech'] if 'tech' in df_sum_epe.index else 0][0],\n",
    "                [df_sum_epe['other'] if 'other' in df_sum_epe.index else 0][0]]   \n",
    "    else:\n",
    "        x_epe = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "        y_epe = [0,0,0,0,0,0]\n",
    "    \n",
    "    if 'The Guardian' in df_sum.index:\n",
    "        \n",
    "        df_sum_thg = df_sum['The Guardian']\n",
    "        x_thg = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "        y_thg = [[df_sum_thg['politics'] if 'politics' in df_sum_thg.index else 0][0],\n",
    "                [df_sum_thg['business'] if 'business' in df_sum_thg.index else 0][0],\n",
    "                [df_sum_thg['entertainment'] if 'entertainment' in df_sum_thg.index else 0][0],\n",
    "                [df_sum_thg['sport'] if 'sport' in df_sum_thg.index else 0][0],\n",
    "                [df_sum_thg['tech'] if 'tech' in df_sum_thg.index else 0][0],\n",
    "                [df_sum_thg['other'] if 'other' in df_sum_thg.index else 0][0]]   \n",
    "    else:\n",
    "        x_thg = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "        y_thg = [0,0,0,0,0,0]\n",
    "\n",
    "    if 'Sky News' in df_sum.index:\n",
    "    \n",
    "        df_sum_skn = df_sum['Sky News']\n",
    "        x_skn = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "        y_skn = [[df_sum_skn['politics'] if 'politics' in df_sum_skn.index else 0][0],\n",
    "                [df_sum_skn['business'] if 'business' in df_sum_skn.index else 0][0],\n",
    "                [df_sum_skn['entertainment'] if 'entertainment' in df_sum_skn.index else 0][0],\n",
    "                [df_sum_skn['sport'] if 'sport' in df_sum_skn.index else 0][0],\n",
    "                [df_sum_skn['tech'] if 'tech' in df_sum_skn.index else 0][0],\n",
    "                [df_sum_skn['other'] if 'other' in df_sum_skn.index else 0][0]]   \n",
    "\n",
    "    else:\n",
    "        x_skn = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "        y_skn = [0,0,0,0,0,0]\n",
    "\n",
    "    # Create plotly figure\n",
    "    figure = {\n",
    "        'data': [\n",
    "            {'x': x_epe, 'y':y_epe, 'type': 'bar', 'name': 'El Pais', 'marker': {'color': 'rgb(62, 137, 195)'}},\n",
    "            {'x': x_thg, 'y':y_thg, 'type': 'bar', 'name': 'The Guardian', 'marker': {'color': 'rgb(167, 203, 232)'}},\n",
    "            {'x': x_skn, 'y':y_skn, 'type': 'bar', 'name': 'Sky News', 'marker': {'color': 'rgb(197, 223, 242)'}}\n",
    "        ],\n",
    "        'layout': {\n",
    "            'title': 'Number of news articles by newspaper',\n",
    "            'plot_bgcolor': colors['graph_background'],\n",
    "            'paper_bgcolor': colors['graph_background'],\n",
    "            'font': {\n",
    "                    'color': colors['text'],\n",
    "                    'size': '10'\n",
    "            },\n",
    "            'barmode': 'stack'\n",
    "            \n",
    "        }   \n",
    "    }\n",
    "\n",
    "    return figure\n",
    "\n",
    "@app.callback(\n",
    "    Output('graph2', 'figure'),\n",
    "    [Input('intermediate-value', 'children')])\n",
    "def update_piechart(jsonified_df):\n",
    "    \n",
    "    df = pd.read_json(jsonified_df, orient='split')\n",
    "    \n",
    "    # Create a summary df\n",
    "    df_sum = df['Prediction'].value_counts()\n",
    "\n",
    "    # Create x and y arrays for the bar plot\n",
    "    x = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "    y = [[df_sum['politics'] if 'politics' in df_sum.index else 0][0],\n",
    "         [df_sum['business'] if 'business' in df_sum.index else 0][0],\n",
    "         [df_sum['entertainment'] if 'entertainment' in df_sum.index else 0][0],\n",
    "         [df_sum['sport'] if 'sport' in df_sum.index else 0][0],\n",
    "         [df_sum['tech'] if 'tech' in df_sum.index else 0][0],\n",
    "         [df_sum['other'] if 'other' in df_sum.index else 0][0]]\n",
    "    \n",
    "    # Create plotly figure\n",
    "    figure = {\n",
    "        'data': [\n",
    "            {'values': y,\n",
    "             'labels': x, \n",
    "             'type': 'pie',\n",
    "             'hole': .4,\n",
    "             'name': '% of news articles',\n",
    "             'marker': {'colors': ['rgb(62, 137, 195)',\n",
    "                                   'rgb(167, 203, 232)',\n",
    "                                   'rgb(197, 223, 242)',\n",
    "                                   'rgb(51, 113, 159)',\n",
    "                                   'rgb(64, 111, 146)',\n",
    "                                   'rgb(31, 84, 132)']},\n",
    "\n",
    "            }\n",
    "        ],\n",
    "        \n",
    "        'layout': {\n",
    "            'title': 'News articles by newspaper',\n",
    "            'plot_bgcolor': colors['graph_background'],\n",
    "            'paper_bgcolor': colors['graph_background'],\n",
    "            'font': {\n",
    "                    'color': colors['text'],\n",
    "                    'size': '10'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    }\n",
    "    \n",
    "    return figure\n",
    "    \n",
    "    \n",
    "@app.callback(\n",
    "    [\n",
    "    Output('textarea1a', 'href'),\n",
    "    Output('textarea1a', 'children'),\n",
    "    Output('textarea1b', 'children'),\n",
    "    Output('textarea2a', 'href'),\n",
    "    Output('textarea2a', 'children'),\n",
    "    Output('textarea2b', 'children'),\n",
    "    Output('textarea3a', 'href'),\n",
    "    Output('textarea3a', 'children'),\n",
    "    Output('textarea3b', 'children'),\n",
    "    Output('textarea4a', 'href'),\n",
    "    Output('textarea4a', 'children'),\n",
    "    Output('textarea4b', 'children'),\n",
    "    Output('textarea5a', 'href'),\n",
    "    Output('textarea5a', 'children'),\n",
    "    Output('textarea5b', 'children'),\n",
    "    Output('textarea6a', 'href'),\n",
    "    Output('textarea6a', 'children'),\n",
    "    Output('textarea6b', 'children'),\n",
    "    Output('textarea7a', 'href'),\n",
    "    Output('textarea7a', 'children'),\n",
    "    Output('textarea7b', 'children'),\n",
    "    Output('textarea8a', 'href'),\n",
    "    Output('textarea8a', 'children'),\n",
    "    Output('textarea8b', 'children'),\n",
    "    Output('textarea9a', 'href'),\n",
    "    Output('textarea9a', 'children'),\n",
    "    Output('textarea9b', 'children'),\n",
    "    Output('textarea10a', 'href'),\n",
    "    Output('textarea10a', 'children'),\n",
    "    Output('textarea10b', 'children'),\n",
    "    Output('textarea11a', 'href'),\n",
    "    Output('textarea11a', 'children'),\n",
    "    Output('textarea11b', 'children'),\n",
    "    Output('textarea12a', 'href'),\n",
    "    Output('textarea12a', 'children'),\n",
    "    Output('textarea12b', 'children'),\n",
    "    Output('textarea13a', 'href'),\n",
    "    Output('textarea13a', 'children'),\n",
    "    Output('textarea13b', 'children'),\n",
    "    Output('textarea14a', 'href'),\n",
    "    Output('textarea14a', 'children'),\n",
    "    Output('textarea14b', 'children'),\n",
    "    Output('textarea15a', 'href'),\n",
    "    Output('textarea15a', 'children'),\n",
    "    Output('textarea15b', 'children')\n",
    "    ],\n",
    "    [Input('intermediate-value', 'children')])\n",
    "def update_textarea1(jsonified_df):\n",
    "    \n",
    "    df = pd.read_json(jsonified_df, orient='split')\n",
    "    \n",
    "    texts = []\n",
    "    links = []\n",
    "    preds_newsp = []\n",
    "    \n",
    "    for article in range(len(df)):\n",
    "        texts.append(df.iloc[article]['Article Title'])\n",
    "        links.append(df.iloc[article]['Article Link'])\n",
    "        preds_newsp.append((df.iloc[article]['Prediction'].capitalize()) + ', ' + (df.iloc[article]['Newspaper']))\n",
    "\n",
    "    while (len(texts) < 16):\n",
    "        texts.append(None)\n",
    "        links.append(None)\n",
    "        preds_newsp.append(None)\n",
    "    \n",
    "    return \\\n",
    "        links[0], texts[0], preds_newsp[0],\\\n",
    "        links[1], texts[1], preds_newsp[1],\\\n",
    "        links[2], texts[2], preds_newsp[2],\\\n",
    "        links[3], texts[3], preds_newsp[3],\\\n",
    "        links[4], texts[4], preds_newsp[4],\\\n",
    "        links[5], texts[5], preds_newsp[5],\\\n",
    "        links[6], texts[6], preds_newsp[6],\\\n",
    "        links[7], texts[7], preds_newsp[7],\\\n",
    "        links[8], texts[8], preds_newsp[8],\\\n",
    "        links[9], texts[9], preds_newsp[9],\\\n",
    "        links[10], texts[10], preds_newsp[10],\\\n",
    "        links[11], texts[11], preds_newsp[11],\\\n",
    "        links[12], texts[12], preds_newsp[12],\\\n",
    "        links[13], texts[13], preds_newsp[13],\\\n",
    "        links[14], texts[14], preds_newsp[14]\n",
    "           \n",
    "    \n",
    "    \n",
    "# Loading CSS\n",
    "app.css.append_css({\"external_url\": \"https://codepen.io/chriddyp/pen/bWLwgP.css\"})\n",
    "app.css.append_css({\"external_url\": \"https://codepen.io/chriddyp/pen/brPBPO.css\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: Do not use the development server in a production environment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8050/ (Press CTRL+C to quit)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\dash\\resources.py:44: UserWarning:\n",
      "\n",
      "A local version of https://codepen.io/chriddyp/pen/bWLwgP.css is not available\n",
      "\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\dash\\resources.py:44: UserWarning:\n",
      "\n",
      "A local version of https://codepen.io/chriddyp/pen/brPBPO.css is not available\n",
      "\n",
      "127.0.0.1 - - [14/Jul/2019 01:14:47] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jul/2019 01:14:48] \"GET /_dash-component-suites/dash_renderer/react@16.8.6.min.js?v=1.0.0&m=1563040120 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jul/2019 01:14:48] \"GET /_dash-component-suites/dash_renderer/prop-types@15.7.2.min.js?v=1.0.0&m=1563040120 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jul/2019 01:14:48] \"GET /_dash-component-suites/dash_renderer/react-dom@16.8.6.min.js?v=1.0.0&m=1563040120 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jul/2019 01:14:48] \"GET /_dash-component-suites/dash_core_components/highlight.pack.js?v=1.0.0&m=1563040122 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jul/2019 01:14:48] \"GET /_dash-component-suites/dash_html_components/dash_html_components.min.js?v=1.0.0&m=1563039993 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jul/2019 01:14:48] \"GET /_dash-component-suites/dash_table/bundle.js?v=4.0.1&m=1563039994 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jul/2019 01:14:48] \"GET /_dash-component-suites/dash_core_components/dash_core_components.min.js?v=1.0.0&m=1563040122 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jul/2019 01:14:48] \"GET /_dash-component-suites/dash_renderer/dash_renderer.min.js?v=1.0.0&m=1563040120 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jul/2019 01:14:48] \"GET /_dash-component-suites/dash_daq/bundle.js?v=0.1.0&m=1563040486 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jul/2019 01:14:48] \"GET /_dash-component-suites/dash_core_components/plotly-1.48.3.min.js?v=1.0.0&m=1563040122 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jul/2019 01:14:48] \"GET /_dash-layout HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jul/2019 01:14:49] \"GET /_dash-dependencies HTTP/1.1\" 200 -\n",
      "[2019-07-14 01:14:49,036] ERROR in app: Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 2292, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1815, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1718, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\_compat.py\", line 35, in reraise\n",
      "    raise value\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1813, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1799, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\dash\\dash.py\", line 1291, in dispatch\n",
      "    response.set_data(self.callback_map[output]['callback'](*args))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\dash\\dash.py\", line 1175, in add_context\n",
      "    output_value = func(*args, **kwargs)\n",
      "  File \"<ipython-input-407-acc1958ec567>\", line 425, in update_textarea1\n",
      "    df = pd.read_json(jsonified_df, orient='split')\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\json.py\", line 413, in read_json\n",
      "    path_or_buf, encoding=encoding, compression=compression,\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\", line 232, in get_filepath_or_buffer\n",
      "    raise ValueError(msg.format(_type=type(filepath_or_buffer)))\n",
      "ValueError: Invalid file path or buffer object type: <class 'NoneType'>\n",
      "127.0.0.1 - - [14/Jul/2019 01:14:49] \"POST /_dash-update-component HTTP/1.1\" 500 -\n",
      "[2019-07-14 01:14:49,038] ERROR in app: Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 2292, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1815, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1718, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\_compat.py\", line 35, in reraise\n",
      "    raise value\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1813, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1799, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\dash\\dash.py\", line 1291, in dispatch\n",
      "    response.set_data(self.callback_map[output]['callback'](*args))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\dash\\dash.py\", line 1175, in add_context\n",
      "    output_value = func(*args, **kwargs)\n",
      "  File \"<ipython-input-407-acc1958ec567>\", line 327, in update_piechart\n",
      "    df = pd.read_json(jsonified_df, orient='split')\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\json.py\", line 413, in read_json\n",
      "    path_or_buf, encoding=encoding, compression=compression,\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\", line 232, in get_filepath_or_buffer\n",
      "    raise ValueError(msg.format(_type=type(filepath_or_buffer)))\n",
      "ValueError: Invalid file path or buffer object type: <class 'NoneType'>\n",
      "127.0.0.1 - - [14/Jul/2019 01:14:49] \"POST /_dash-update-component HTTP/1.1\" 500 -\n",
      "[2019-07-14 01:14:49,040] ERROR in app: Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 2292, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1815, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1718, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\_compat.py\", line 35, in reraise\n",
      "    raise value\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1813, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1799, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\dash\\dash.py\", line 1291, in dispatch\n",
      "    response.set_data(self.callback_map[output]['callback'](*args))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\dash\\dash.py\", line 1175, in add_context\n",
      "    output_value = func(*args, **kwargs)\n",
      "  File \"<ipython-input-407-acc1958ec567>\", line 251, in update_barchart\n",
      "    df = pd.read_json(jsonified_df, orient='split')\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\json.py\", line 413, in read_json\n",
      "    path_or_buf, encoding=encoding, compression=compression,\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\", line 232, in get_filepath_or_buffer\n",
      "    raise ValueError(msg.format(_type=type(filepath_or_buffer)))\n",
      "ValueError: Invalid file path or buffer object type: <class 'NoneType'>\n",
      "127.0.0.1 - - [14/Jul/2019 01:14:49] \"POST /_dash-update-component HTTP/1.1\" 500 -\n",
      "127.0.0.1 - - [14/Jul/2019 01:14:53] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jul/2019 01:14:53] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jul/2019 01:14:53] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jul/2019 01:14:53] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jul/2019 01:15:37] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jul/2019 01:15:38] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jul/2019 01:15:38] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Jul/2019 01:15:38] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app.run_server(debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
