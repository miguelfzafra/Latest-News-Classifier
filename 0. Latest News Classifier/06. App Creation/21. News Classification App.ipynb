{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Classification App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "* https://www.w3schools.com/colors/colors_picker.asp\n",
    "* https://stackoverflow.com/questions/47949173/deploy-a-python-app-to-heroku-using-conda-environments-instead-of-virtualenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import punkt\n",
    "from nltk.corpus.reader import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import dash_table\n",
    "from dash.dependencies import Input, Output, State, Event\n",
    "import plotly.graph_objs as go\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performing model is the SVM. We'll use it in the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_models = \"/home/lnc/0. Latest News Classifier/04. Model Training/Models/\"\n",
    "\n",
    "# SVM\n",
    "path_svm = path_models + 'best_svc.pickle'\n",
    "with open(path_svm, 'rb') as data:\n",
    "    svc_model = pickle.load(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. TF-IDF object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tfidf = \"/home/lnc/0. Latest News Classifier/03. Feature Engineering/Pickles/tfidf.pickle\"\n",
    "\n",
    "with open(path_tfidf, 'rb') as data:\n",
    "    tfidf = pickle.load(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Category mapping dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_codes = {\n",
    "    'business': 0,\n",
    "    'entertainment': 1,\n",
    "    'politics': 2,\n",
    "    'sport': 3,\n",
    "    'tech': 4,\n",
    "    'other':5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Definition of functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Web Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El Pais\n",
    "def get_news_elpais():\n",
    "    \n",
    "    # url definition\n",
    "    url = \"https://elpais.com/elpais/inenglish.html\"\n",
    "    \n",
    "    # Request\n",
    "    r1 = requests.get(url)\n",
    "    r1.status_code\n",
    "\n",
    "    # We'll save in coverpage the cover page content\n",
    "    coverpage = r1.content\n",
    "\n",
    "    # Soup creation\n",
    "    soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "\n",
    "    # News identification\n",
    "    coverpage_news = soup1.find_all('h2', class_='articulo-titulo')\n",
    "    len(coverpage_news)\n",
    "    \n",
    "    number_of_articles = 5\n",
    "\n",
    "    # Empty lists for content, links and titles\n",
    "    news_contents = []\n",
    "    list_links = []\n",
    "    list_titles = []\n",
    "\n",
    "    for n in np.arange(0, number_of_articles):\n",
    "\n",
    "        # only news articles (there are also albums and other things)\n",
    "        if \"inenglish\" not in coverpage_news[n].find('a')['href']:  \n",
    "            continue\n",
    "\n",
    "        # Getting the link of the article\n",
    "        link = coverpage_news[n].find('a')['href']\n",
    "        list_links.append(link)\n",
    "\n",
    "        # Getting the title\n",
    "        title = coverpage_news[n].find('a').get_text()\n",
    "        list_titles.append(title)\n",
    "\n",
    "        # Reading the content (it is divided in paragraphs)\n",
    "        article = requests.get(link)\n",
    "        article_content = article.content\n",
    "        soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "        body = soup_article.find_all('div', class_='articulo-cuerpo')\n",
    "        x = body[0].find_all('p')\n",
    "\n",
    "        # Unifying the paragraphs\n",
    "        list_paragraphs = []\n",
    "        for p in np.arange(0, len(x)):\n",
    "            paragraph = x[p].get_text()\n",
    "            list_paragraphs.append(paragraph)\n",
    "            final_article = \" \".join(list_paragraphs)\n",
    "\n",
    "        news_contents.append(final_article)\n",
    "\n",
    "    # df_features\n",
    "    df_features = pd.DataFrame(\n",
    "         {'Content': news_contents \n",
    "        })\n",
    "\n",
    "    # df_show_info\n",
    "    df_show_info = pd.DataFrame(\n",
    "        {'Article Title': list_titles,\n",
    "         'Article Link': list_links,\n",
    "         'Newspaper': 'El Pais English'})\n",
    "    \n",
    "    return (df_features, df_show_info)\n",
    "\n",
    "# The Guardian\n",
    "def get_news_theguardian():\n",
    "    \n",
    "    # url definition\n",
    "    url = \"https://www.theguardian.com/uk\"\n",
    "    \n",
    "    # Request\n",
    "    r1 = requests.get(url)\n",
    "    r1.status_code\n",
    "\n",
    "    # We'll save in coverpage the cover page content\n",
    "    coverpage = r1.content\n",
    "\n",
    "    # Soup creation\n",
    "    soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "\n",
    "    # News identification\n",
    "    coverpage_news = soup1.find_all('h3', class_='fc-item__title')\n",
    "    len(coverpage_news)\n",
    "    \n",
    "    number_of_articles = 5\n",
    "\n",
    "    # Empty lists for content, links and titles\n",
    "    news_contents = []\n",
    "    list_links = []\n",
    "    list_titles = []\n",
    "\n",
    "    for n in np.arange(0, number_of_articles):\n",
    "\n",
    "        # We need to ignore \"live\" pages since they are not articles\n",
    "        if \"live\" in coverpage_news[n].find('a')['href']:  \n",
    "            continue\n",
    "            \n",
    "        # We also need to ignore \"commentisfree\" pages\n",
    "        if \"commentisfree\" in coverpage_news[n].find('a')['href']:  \n",
    "            continue\n",
    "            \n",
    "        # And \"ng-interactive\" pages\n",
    "        if \"ng-interactive\" in coverpage_news[n].find('a')['href']:  \n",
    "            continue\n",
    "\n",
    "        # Getting the link of the article\n",
    "        link = coverpage_news[n].find('a')['href']\n",
    "        list_links.append(link)\n",
    "\n",
    "        # Getting the title\n",
    "        title = coverpage_news[n].find('a').get_text()\n",
    "        list_titles.append(title)\n",
    "\n",
    "        # Reading the content (it is divided in paragraphs)\n",
    "        article = requests.get(link)\n",
    "        article_content = article.content\n",
    "        soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "        body = soup_article.find_all('div', class_='content__article-body from-content-api js-article__body')\n",
    "        x = body[0].find_all('p')\n",
    "\n",
    "        # Unifying the paragraphs\n",
    "        list_paragraphs = []\n",
    "        for p in np.arange(0, len(x)):\n",
    "            paragraph = x[p].get_text()\n",
    "            list_paragraphs.append(paragraph)\n",
    "            final_article = \" \".join(list_paragraphs)\n",
    "\n",
    "        news_contents.append(final_article)\n",
    "\n",
    "    # df_features\n",
    "    df_features = pd.DataFrame(\n",
    "         {'Content': news_contents \n",
    "        })\n",
    "\n",
    "    # df_show_info\n",
    "    df_show_info = pd.DataFrame(\n",
    "        {'Article Title': list_titles,\n",
    "         'Article Link': list_links,\n",
    "         'Newspaper': 'The Guardian'})\n",
    "\n",
    "    \n",
    "    return (df_features, df_show_info)\n",
    "\n",
    "# The Mirror\n",
    "def get_news_themirror():\n",
    "    \n",
    "    # url definition\n",
    "    url = \"https://www.mirror.co.uk/\"\n",
    "    \n",
    "    # Request\n",
    "    r1 = requests.get(url)\n",
    "    r1.status_code\n",
    "\n",
    "    # We'll save in coverpage the cover page content\n",
    "    coverpage = r1.content\n",
    "\n",
    "    # Soup creation\n",
    "    soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "\n",
    "    # News identification\n",
    "    coverpage_news = soup1.find_all('a', class_='headline publication-font')\n",
    "    len(coverpage_news)\n",
    "    \n",
    "    number_of_articles = 5\n",
    "\n",
    "    # Empty lists for content, links and titles\n",
    "    news_contents = []\n",
    "    list_links = []\n",
    "    list_titles = []\n",
    "\n",
    "    for n in np.arange(0, number_of_articles):\n",
    "\n",
    "        # Getting the link of the article\n",
    "        link = coverpage_news[n]['href']\n",
    "        list_links.append(link)\n",
    "\n",
    "        # Getting the title\n",
    "        title = coverpage_news[n].get_text()\n",
    "        list_titles.append(title)\n",
    "\n",
    "        # Reading the content (it is divided in paragraphs)\n",
    "        article = requests.get(link)\n",
    "        article_content = article.content\n",
    "        soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "        body = soup_article.find_all('div', class_='articulo-cuerpo')\n",
    "        x = soup_article.find_all('p')\n",
    "\n",
    "        # Unifying the paragraphs\n",
    "        list_paragraphs = []\n",
    "        for p in np.arange(0, len(x)):\n",
    "            paragraph = x[p].get_text()\n",
    "            list_paragraphs.append(paragraph)\n",
    "            final_article = \" \".join(list_paragraphs)\n",
    "\n",
    "        news_contents.append(final_article)\n",
    "\n",
    "    # df_features\n",
    "    df_features = pd.DataFrame(\n",
    "         {'Content': news_contents \n",
    "        })\n",
    "\n",
    "    # df_show_info\n",
    "    df_show_info = pd.DataFrame(\n",
    "        {'Article Title': list_titles,\n",
    "         'Article Link': list_links,\n",
    "         'Newspaper': 'The Mirror'})\n",
    "    \n",
    "    return (df_features, df_show_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_signs = list(\"?:!.,;\")\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "def create_features_from_df(df):\n",
    "    \n",
    "    df['Content_Parsed_1'] = df['Content'].str.replace(\"\\r\", \" \")\n",
    "    df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"\\n\", \" \")\n",
    "    df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"    \", \" \")\n",
    "    df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace('\"', '')\n",
    "    \n",
    "    df['Content_Parsed_2'] = df['Content_Parsed_1'].str.lower()\n",
    "    \n",
    "    df['Content_Parsed_3'] = df['Content_Parsed_2']\n",
    "    for punct_sign in punctuation_signs:\n",
    "        df['Content_Parsed_3'] = df['Content_Parsed_3'].str.replace(punct_sign, '')\n",
    "        \n",
    "    df['Content_Parsed_4'] = df['Content_Parsed_3'].str.replace(\"'s\", \"\")\n",
    "    \n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    nrows = len(df)\n",
    "    lemmatized_text_list = []\n",
    "    for row in range(0, nrows):\n",
    "\n",
    "        # Create an empty list containing lemmatized words\n",
    "        lemmatized_list = []\n",
    "        # Save the text and its words into an object\n",
    "        text = df.loc[row]['Content_Parsed_4']\n",
    "        text_words = text.split(\" \")\n",
    "        # Iterate through every word to lemmatize\n",
    "        for word in text_words:\n",
    "            lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "        # Join the list\n",
    "        lemmatized_text = \" \".join(lemmatized_list)\n",
    "        # Append to the list containing the texts\n",
    "        lemmatized_text_list.append(lemmatized_text)\n",
    "    \n",
    "    df['Content_Parsed_5'] = lemmatized_text_list\n",
    "    \n",
    "    df['Content_Parsed_6'] = df['Content_Parsed_5']\n",
    "    for stop_word in stop_words:\n",
    "        regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
    "        df['Content_Parsed_6'] = df['Content_Parsed_6'].str.replace(regex_stopword, '')\n",
    "        \n",
    "    df = df['Content_Parsed_6']\n",
    "    df = df.rename(columns={'Content_Parsed_6': 'Content_Parsed'})\n",
    "    \n",
    "    # TF-IDF\n",
    "    features = tfidf.transform(df).toarray()\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_name(category_id):\n",
    "    for category, id_ in category_codes.items():    \n",
    "        if id_ == category_id:\n",
    "            return category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Prediction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_features(features):\n",
    "        \n",
    "    # Obtain the highest probability of the predictions for each article\n",
    "    predictions_proba = svc_model.predict_proba(features).max(axis=1)    \n",
    "    \n",
    "    # Predict using the input model\n",
    "    predictions_pre = svc_model.predict(features)\n",
    "\n",
    "    # Replace prediction with 6 if associated cond. probability less than threshold\n",
    "    predictions = []\n",
    "\n",
    "    for prob, cat in zip(predictions_proba, predictions_pre):\n",
    "        if prob > .65:\n",
    "            predictions.append(cat)\n",
    "        else:\n",
    "            predictions.append(5)\n",
    "\n",
    "    # Return result\n",
    "    categories = [get_category_name(x) for x in predictions]\n",
    "    \n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_df(df, categories):\n",
    "    df['Prediction'] = categories\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the whole process can be written in these 4 lines of code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Get the scraped dataframes\n",
    "df_features, df_show_info = get_news_elpais()\n",
    "\n",
    "# Create features\n",
    "features = create_features_from_df(df_features)\n",
    "\n",
    "# Predict\n",
    "predictions = predict_from_features(features)\n",
    "\n",
    "# Put into dataset\n",
    "df = complete_df(df_show_info, predictions)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dash App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stylesheet\n",
    "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "\n",
    "app = dash.Dash(__name__, external_stylesheets=external_stylesheets)\n",
    "\n",
    "# Colors\n",
    "colors = {\n",
    "    'background': '#ffffff',\n",
    "    'text': '#696969',\n",
    "    'header_table': '#ffedb3'\n",
    "}\n",
    "\n",
    "# Markdown text\n",
    "markdown_text1 = '''\n",
    "\n",
    "This application gathers the latest news from the newspapers **El Pais**, **The Guardian** and **The Mirror**, predicts their category between **Politics**, **Business**, **Entertainment**, **Sport**, **Tech** and **Other** and then shows a graphic summary.\n",
    "\n",
    "The news categories are predicted with a Support Vector Machine model.\n",
    "\n",
    "Please enter which newspapers would you like to scrape news off and press **Submit**:\n",
    "\n",
    "'''\n",
    "\n",
    "markdown_text2 = '''\n",
    "\n",
    "*The scraped news are converted into a numeric feature vector with TF-IDF vectorization. Then, a Support Vector Classifier is applied to predict each category.*\n",
    "\n",
    "\\n\n",
    "\\n\n",
    "Warning: The Mirror takes approximately 30 seconds to gather the news articles.\n",
    "\\n\n",
    "Created by Miguel Fern√°ndez Zafra.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "app.layout = html.Div(style={'backgroundColor':colors['background']}, children=[\n",
    "    \n",
    "    # Title\n",
    "    html.H1(children='News Classification App',\n",
    "            style={\n",
    "                'textAlign': 'left',\n",
    "                'color': colors['text'],\n",
    "                'padding': '20px',\n",
    "                'backgroundColor': colors['header_table']\n",
    "\n",
    "            },\n",
    "            className='banner',\n",
    "           ),\n",
    "\n",
    "    # Sub-title Left\n",
    "    html.Div([\n",
    "        dcc.Markdown(children=markdown_text1)],\n",
    "        style={'width': '49%', 'display': 'inline-block'}),\n",
    "    \n",
    "    # Sub-title Right\n",
    "    html.Div([\n",
    "        dcc.Markdown(children=markdown_text2)],\n",
    "        style={'width': '49%', 'float': 'right', 'display': 'inline-block'}),\n",
    "\n",
    "    # Space between text and dropdown\n",
    "    html.H1(id='space', children=' '),\n",
    "\n",
    "    # Dropdown\n",
    "    html.Div([\n",
    "        dcc.Dropdown(\n",
    "            options=[\n",
    "                {'label': 'El Pais English', 'value': 'EPE'},\n",
    "                {'label': 'The Guardian', 'value': 'THG'},\n",
    "                {'label': 'The Mirror', 'value': 'TMI'}\n",
    "            ],\n",
    "            value=['EPE', 'THG'],\n",
    "            multi=True,\n",
    "            id='checklist')],\n",
    "        style={'width': '40%', 'display': 'inline-block', 'float': 'left'}),\n",
    "        \n",
    "\n",
    "    # Button\n",
    "    html.Div([\n",
    "        html.Button('Submit', id='submit', type='submit')],\n",
    "        style={'float': 'center'}),\n",
    "    \n",
    "    # Output Block\n",
    "    html.Div(id='output-container-button', children=' '),\n",
    "    \n",
    "    # Graph1\n",
    "    html.Div([\n",
    "        dcc.Graph(id='graph1')],\n",
    "        style={'width': '49%', 'display': 'inline-block'}),\n",
    "    \n",
    "    # Graph2\n",
    "    html.Div([\n",
    "        dcc.Graph(id='graph2')],\n",
    "        style={'width': '49%', 'float': 'right', 'display': 'inline-block'}),\n",
    "    \n",
    "    # Table title\n",
    "    html.Div(id='table-title', children='You can see a summary of the news articles below:'),\n",
    "\n",
    "    # Space\n",
    "    html.H1(id='space2', children=' '),\n",
    "    \n",
    "    # Table\n",
    "    html.Div([\n",
    "        dash_table.DataTable(\n",
    "            id='table',\n",
    "            columns=[{\"name\": i, \"id\": i} for i in ['Article Title', 'Article Link', 'Newspaper', 'Prediction']],\n",
    "            style_data={'whiteSpace': 'normal'},\n",
    "            style_as_list_view=True,\n",
    "            style_cell={'padding': '5px', 'textAlign': 'left', 'backgroundColor': colors['background']},\n",
    "            style_header={\n",
    "                'backgroundColor': colors ['header_table'],\n",
    "                'fontWeight': 'bold'\n",
    "            },\n",
    "            style_table={\n",
    "                'maxHeight': '300',\n",
    "                'overflowY':'scroll'\n",
    "                \n",
    "            },\n",
    "            css=[{\n",
    "                'selector': '.dash-cell div.dash-cell-value',\n",
    "                'rule': 'display: inline; white-space: inherit; overflow: inherit; text-overflow: inherit;'\n",
    "            }]\n",
    "        )],\n",
    "        style={'width': '75%','float': 'left', 'position': 'relative', 'left': '12.5%', 'display': 'inline-block'}),    \n",
    "        \n",
    "    # Hidden div inside the app that stores the intermediate value\n",
    "    html.Div(id='intermediate-value', style={'display': 'none'})\n",
    "    \n",
    "\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output('intermediate-value', 'children'),\n",
    "    [],\n",
    "    [State('checklist', 'value')],\n",
    "    [Event('submit', 'click')])\n",
    "def scrape_and_predict(values):\n",
    "    \n",
    "    df_features = pd.DataFrame()\n",
    "    df_show_info = pd.DataFrame()\n",
    "    \n",
    "    if 'EPE' in values:\n",
    "        # Get the scraped dataframes\n",
    "        df_features = df_features.append(get_news_elpais()[0])\n",
    "        df_show_info = df_show_info.append(get_news_elpais()[1])\n",
    "    \n",
    "    if 'THG' in values:\n",
    "        df_features = df_features.append(get_news_theguardian()[0])\n",
    "        df_show_info = df_show_info.append(get_news_theguardian()[1])\n",
    "        \n",
    "    if 'TMI' in values:\n",
    "        df_features = df_features.append(get_news_themirror()[0])\n",
    "        df_show_info = df_show_info.append(get_news_themirror()[1])\n",
    "\n",
    "    df_features = df_features.reset_index().drop('index', axis=1)\n",
    "    \n",
    "    # Create features\n",
    "    features = create_features_from_df(df_features)\n",
    "    # Predict\n",
    "    predictions = predict_from_features(features)\n",
    "    # Put into dataset\n",
    "    df = complete_df(df_show_info, predictions)\n",
    "    # df.to_csv('Tableau Teaser/df_tableau.csv', sep='^')  # export to csv to work out an example in Tableau\n",
    "    \n",
    "    return df.to_json(date_format='iso', orient='split')\n",
    "\n",
    "@app.callback(\n",
    "    Output('graph1', 'figure'),\n",
    "    [Input('intermediate-value', 'children')])\n",
    "def update_barchart(jsonified_df):\n",
    "    \n",
    "    df = pd.read_json(jsonified_df, orient='split')\n",
    "    \n",
    "    # Create a summary df\n",
    "    df_sum = df.groupby(['Newspaper', 'Prediction']).count()['Article Title']\n",
    "\n",
    "    # Create x and y arrays for the bar plot for every newspaper\n",
    "    if 'El Pais English' in df_sum.index:\n",
    "    \n",
    "        df_sum_epe = df_sum['El Pais English']\n",
    "        x_epe = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "        y_epe = [[df_sum_epe['politics'] if 'politics' in df_sum_epe.index else 0][0],\n",
    "                [df_sum_epe['business'] if 'business' in df_sum_epe.index else 0][0],\n",
    "                [df_sum_epe['entertainment'] if 'entertainment' in df_sum_epe.index else 0][0],\n",
    "                [df_sum_epe['sport'] if 'sport' in df_sum_epe.index else 0][0],\n",
    "                [df_sum_epe['tech'] if 'tech' in df_sum_epe.index else 0][0],\n",
    "                [df_sum_epe['other'] if 'other' in df_sum_epe.index else 0][0]]   \n",
    "    else:\n",
    "        x_epe = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "        y_epe = [0,0,0,0,0,0]\n",
    "    \n",
    "    if 'The Guardian' in df_sum.index:\n",
    "        \n",
    "        df_sum_thg = df_sum['The Guardian']\n",
    "        x_thg = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "        y_thg = [[df_sum_thg['politics'] if 'politics' in df_sum_thg.index else 0][0],\n",
    "                [df_sum_thg['business'] if 'business' in df_sum_thg.index else 0][0],\n",
    "                [df_sum_thg['entertainment'] if 'entertainment' in df_sum_thg.index else 0][0],\n",
    "                [df_sum_thg['sport'] if 'sport' in df_sum_thg.index else 0][0],\n",
    "                [df_sum_thg['tech'] if 'tech' in df_sum_thg.index else 0][0],\n",
    "                [df_sum_thg['other'] if 'other' in df_sum_thg.index else 0][0]]   \n",
    "    else:\n",
    "        x_thg = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "        y_thg = [0,0,0,0,0,0]\n",
    "\n",
    "    if 'The Mirror' in df_sum.index:\n",
    "    \n",
    "        df_sum_tmi = df_sum['The Mirror']\n",
    "        x_tmi = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "        y_tmi = [[df_sum_tmi['politics'] if 'politics' in df_sum_tmi.index else 0][0],\n",
    "                [df_sum_tmi['business'] if 'business' in df_sum_tmi.index else 0][0],\n",
    "                [df_sum_tmi['entertainment'] if 'entertainment' in df_sum_tmi.index else 0][0],\n",
    "                [df_sum_tmi['sport'] if 'sport' in df_sum_tmi.index else 0][0],\n",
    "                [df_sum_tmi['tech'] if 'tech' in df_sum_tmi.index else 0][0],\n",
    "                [df_sum_tmi['other'] if 'other' in df_sum_tmi.index else 0][0]]   \n",
    "\n",
    "    else:\n",
    "        x_tmi = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "        y_tmi = [0,0,0,0,0,0]\n",
    "\n",
    "    # Create plotly figure\n",
    "    figure = {\n",
    "        'data': [\n",
    "            {'x': x_epe, 'y':y_epe, 'type': 'bar', 'name': 'El Pais'},\n",
    "            {'x': x_thg, 'y':y_thg, 'type': 'bar', 'name': 'The Guardian'},\n",
    "            {'x': x_tmi, 'y':y_tmi, 'type': 'bar', 'name': 'The Mirror'}\n",
    "        ],\n",
    "        'layout': {\n",
    "            'title': 'Number of news articles by newspaper',\n",
    "            'plot_bgcolor': colors['background'],\n",
    "            'paper_bgcolor': colors['background'],\n",
    "            'font': {\n",
    "                    'color': colors['text']\n",
    "            }\n",
    "        }   \n",
    "    }\n",
    "\n",
    "    return figure\n",
    "\n",
    "@app.callback(\n",
    "    Output('graph2', 'figure'),\n",
    "    [Input('intermediate-value', 'children')])\n",
    "def update_piechart(jsonified_df):\n",
    "    \n",
    "    df = pd.read_json(jsonified_df, orient='split')\n",
    "    \n",
    "    # Create a summary df\n",
    "    df_sum = df['Prediction'].value_counts()\n",
    "\n",
    "    # Create x and y arrays for the bar plot\n",
    "    x = ['Politics', 'Business', 'Entertainment', 'Sport', 'Tech', 'Other']\n",
    "    y = [[df_sum['politics'] if 'politics' in df_sum.index else 0][0],\n",
    "         [df_sum['business'] if 'business' in df_sum.index else 0][0],\n",
    "         [df_sum['entertainment'] if 'entertainment' in df_sum.index else 0][0],\n",
    "         [df_sum['sport'] if 'sport' in df_sum.index else 0][0],\n",
    "         [df_sum['tech'] if 'tech' in df_sum.index else 0][0],\n",
    "         [df_sum['other'] if 'other' in df_sum.index else 0][0]]\n",
    "    \n",
    "    # Create plotly figure\n",
    "    figure = {\n",
    "        'data': [\n",
    "            {'values': y,\n",
    "             'labels': x, \n",
    "             'type': 'pie',\n",
    "             'hole': .4,\n",
    "             'name': '% of news articles'}\n",
    "        ],\n",
    "        \n",
    "        'layout': {\n",
    "            'title': '% of news articles',\n",
    "            'plot_bgcolor': colors['background'],\n",
    "            'paper_bgcolor': colors['background'],\n",
    "            'font': {\n",
    "                    'color': colors['text']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    }\n",
    "    \n",
    "    return figure\n",
    "    \n",
    "    \n",
    "@app.callback(\n",
    "    Output('table', 'data'),\n",
    "    [Input('intermediate-value', 'children')])\n",
    "def update_table(jsonified_df):\n",
    "    \n",
    "    df = pd.read_json(jsonified_df, orient='split')\n",
    "    data = df.to_dict('rows')\n",
    "    return data\n",
    "\n",
    "    \n",
    "    \n",
    "# Loading CSS\n",
    "app.css.append_css({\"external_url\": \"https://codepen.io/chriddyp/pen/bWLwgP.css\"})\n",
    "app.css.append_css({\"external_url\": \"https://codepen.io/chriddyp/pen/brPBPO.css\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: Do not use the development server in a production environment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8050/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "app.run_server(debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
